{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7324ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.features.features import ClassLabel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-PER', 'B-LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding_dict = {'O':0,'B-PER':1,'B-LOC':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d98d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def read_patent_folders(folder_name, test_set):\n",
    "  \n",
    "  train_patents = []\n",
    "  test_patents = []\n",
    "\n",
    "  filenames = sorted(os.listdir(folder_name))\n",
    "\n",
    "  for filename in filenames:\n",
    "    with open(os.path.join(folder_name, filename), 'r') as f: # open in readonly mode\n",
    "        lines = f.readlines()\n",
    "        if filename in test_set:\n",
    "          test_patents.append(lines)\n",
    "        else:\n",
    "          train_patents.append(lines)\n",
    "  \n",
    "  return train_patents, test_patents\n",
    "  \n",
    "\n",
    "def read_patents(folder_name, test_file_id):\n",
    "  \n",
    "  train_patents = []\n",
    "  test_patents = []\n",
    "\n",
    "  filenames = sorted(os.listdir(folder_name))\n",
    "\n",
    "  for filename in filenames:\n",
    "    with open(os.path.join(folder_name, filename), 'r') as f: # open in readonly mode\n",
    "        lines = f.readlines()\n",
    "        if (filename in test_file_id):\n",
    "          test_patents.append(lines)\n",
    "        else:\n",
    "          train_patents.append(lines)\n",
    "  \n",
    "  return train_patents, test_patents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484eb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset(corpus, tags):\n",
    "  \n",
    "  T_prime = []\n",
    "  L_prime = []\n",
    "  input_seqs = []\n",
    "  labels = []\n",
    "  total_length = 0\n",
    "\n",
    "  for word, label in zip(corpus, tags):\n",
    "    tokens = tokenizer([word], is_split_into_words=True, add_special_tokens=False).input_ids\n",
    "    cur_length = len(tokens)\n",
    "    \n",
    "    if (total_length + cur_length) > (MAX_SEQ_LENGTH-2):\n",
    "        \n",
    "      # add new sequence to the list\n",
    "      input_seqs.append(T_prime)\n",
    "      labels.append(L_prime)\n",
    "    \n",
    "      T_prime = []\n",
    "      L_prime = []\n",
    "    \n",
    "      total_length = 0\n",
    "\n",
    "    T_prime.append(word)\n",
    "    L_prime.append(label)\n",
    "    \n",
    "    total_length+=cur_length\n",
    "\n",
    "  input_seqs.append(T_prime)\n",
    "  labels.append(L_prime)\n",
    "\n",
    "  return input_seqs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2288063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_ner(label):\n",
    "  dict1 = {'O':'O','B':'B-PER','I':'B-LOC'}\n",
    "  return dict1[label]\n",
    "  \n",
    "\n",
    "def return_sequences(patents):\n",
    "    \n",
    "  corpus = []\n",
    "  tags = []\n",
    "\n",
    "  input_seqs = []\n",
    "  labels = []\n",
    "\n",
    "  for patent in patents:\n",
    "    # split each patent to get the words and labels\n",
    "    for line in patent:\n",
    "      tmp = line.split('\\t')\n",
    "      corpus.append(tmp[0])\n",
    "      tags.append(label_to_ner(tmp[2].rstrip()))\n",
    "    \n",
    "    X, Y = make_dataset_optimized(corpus, tags)\n",
    "    input_seqs+=(X)\n",
    "    labels+=(Y)\n",
    "    \n",
    "  return input_seqs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_optimized(corpus, tags):\n",
    "  \n",
    "  T_prime = []\n",
    "  L_prime = []\n",
    "  input_seqs = []\n",
    "  labels = []\n",
    "  \n",
    "  tokenized_corpus = tokenizer(corpus, is_split_into_words=True, add_special_tokens=False)\n",
    "  word_ids = tokenized_corpus.word_ids()\n",
    "\n",
    "  token_length=0\n",
    "  cur_index=0\n",
    "  total_length = 0\n",
    "\n",
    "  for elem in word_ids:\n",
    "    if elem == cur_index:\n",
    "      token_length+=1\n",
    "    \n",
    "    else:\n",
    "      if (total_length + token_length) > (MAX_SEQ_LENGTH-2):\n",
    "        # add new sequence to the list\n",
    "        input_seqs.append(T_prime)\n",
    "        labels.append(L_prime)\n",
    "        T_prime = []\n",
    "        L_prime = []\n",
    "        total_length = 0\n",
    "      \n",
    "      T_prime.append(corpus[cur_index])\n",
    "      L_prime.append(tags[cur_index])\n",
    "      total_length+=token_length\n",
    "      token_length = 1\n",
    "      cur_index = elem\n",
    "\n",
    "  T_prime.append(corpus[cur_index])\n",
    "  L_prime.append(tags[cur_index])\n",
    "  input_seqs.append(T_prime)\n",
    "  labels.append(L_prime)\n",
    "\n",
    "  return input_seqs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample(input_seqs, labels):\n",
    "    \n",
    "    pos_input_seqs = []\n",
    "    pos_labels = []\n",
    "    \n",
    "    neg_input_seqs = []\n",
    "    neg_labels = []\n",
    "    \n",
    "    for i in range(0, len(labels)):\n",
    "        if ('B-PER' in labels[i] or 'B-LOC' in labels[i]):\n",
    "            pos_input_seqs.append(input_seqs[i])\n",
    "            pos_labels.append(labels[i])\n",
    "            \n",
    "        else:\n",
    "            neg_input_seqs.append(input_seqs[i])\n",
    "            neg_labels.append(labels[i])            \n",
    "\n",
    "    return pos_input_seqs, pos_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ca073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), padding='max_length', truncation=True, max_length=512,\n",
    "                                 is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == '0':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \n",
    "            \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"],\n",
    "           \"prec_B\":results[\"PER\"][\"precision\"], \"recall_B\":results[\"PER\"][\"recall\"], \"B_count\":results[\"PER\"][\"number\"],\n",
    "           \"prec_I\":results[\"LOC\"][\"precision\"], \"recall_I\":results[\"LOC\"][\"recall\"], \"I_count\":results[\"LOC\"][\"number\"]}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "\n",
    "# model_checkpoint = \"./bert-base-cased\"\n",
    "model_checkpoint = \"./bert-for-patents\"\n",
    "# model_checkpoint = \"allenai/scibert_scivocab_cased\"\n",
    "# model_checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "# model_checkpoint = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "# model_checkpoint = \"./PatentBERT\"\n",
    "batch_size = 16\n",
    "\n",
    "label_all_tokens=False\n",
    "DOWN_SAMPLE=True\n",
    "LOOCV=True\n",
    "\n",
    "data_path = 'patents22/'\n",
    "\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "max_length=512\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding='max_length', max_length=512, \n",
    "                                                    label_pad_token_id=-100)\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ada45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_test_data(test_data):\n",
    "    \n",
    "    train_patents, test_patents = read_patents(data_path, test_data)\n",
    "    \n",
    "    # Train dataset\n",
    "    input_seqs_train, labels_train = return_sequences(train_patents)\n",
    "    if DOWN_SAMPLE==True:\n",
    "        input_seqs_train, labels_train = down_sample(input_seqs_train, labels_train)\n",
    "    pd_train = pd.DataFrame([input_seqs_train, labels_train], index=['tokens', 'ner_tags']).transpose()\n",
    "\n",
    "    # Test dataset\n",
    "    input_seqs_test, labels_test = return_sequences(test_patents)\n",
    "    if DOWN_SAMPLE==True:\n",
    "        input_seqs_test, labels_test = down_sample(input_seqs_test, labels_test)\n",
    "    pd_test = pd.DataFrame([input_seqs_test, labels_test], index=['tokens', 'ner_tags']).transpose()\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(pd_train)\n",
    "    test_dataset = Dataset.from_pandas(pd_test)\n",
    "    \n",
    "    tokenized_patent_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    tokenized_patent_test = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "       \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tokenized_patent_train,\n",
    "        eval_dataset=tokenized_patent_test,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics)\n",
    "    \n",
    "    trainer.train()    \n",
    "    trainer.evaluate()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ba07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patents, test_patents = read_patents(data_path, 'KLJJJ')\n",
    "    \n",
    "# Train dataset\n",
    "input_seqs_train, labels_train = return_sequences(train_patents)\n",
    "print(len(input_seqs_train))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adcc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf35b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8168418B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8088361B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f75fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8168418B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8058419B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a57537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US7892537B1.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US7943822B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40771e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8048987B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US7972611B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8092995B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8106171B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b59fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8124829B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b685324",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8133710B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeafb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8148089B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8158348B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8158424B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd900b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8227661B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e060e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8258289B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8273354B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85734b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8293506B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8299100B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8338131B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8409856B2.bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOOCV==True:\n",
    "    train_model_with_test_data('US8114637B2.bio')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
